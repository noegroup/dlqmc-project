I didn’t have much success with the things I’ve tried. my goal is to be able to go to larger learning rates, because that seems to give better converged energies, while avoiding the blowup of the network. based on the tips by Jonas, I’ve tried playing around with various kinds of normalization. as I semi expected, the batch normalization doesn’t seem to work, making the training much less stable. I assume that it’s because of the outliers—we deal with them on the level of the local energy, but not in the activations throughout the network. so if there is one bad outlier, the batch norm can propagate it to the rest of the network. I’ve tried placing the batch norm in different places of our architecture, but didn’t seem to make a difference. I’ve also tried layer normalization, and that didn’t cause any problems, but also didn’t improve anything in terms of stability for larger learning rates. the different normalization also forced me to add various scaling factors in the architecture (like multiple the schnet message by 0.1, etc.), to ensure that the initialized ansatz is not too far from the HF solution. and this brought me back to KFAC and natural gradient. I can see how the different scales in the architecture influence the behavior. but it’s impossible to know what the right scale is. now, I think I have a much better understanding of the natural gradient than I had before, so I’ve wanted to try the KFAC or something like that again. Even just using the diagonal of the Fischer matrix (which is computationally easy) would give us the right covariant scale for all the parameters. I now also understand what I wasn’t doing right before with KFAC. for example, I wasn’t preconditioning the embeddings in schnet, which are crucial. so this had forced me to basically overdamp KFAC, which essentially makes it an identity preconditioner.
