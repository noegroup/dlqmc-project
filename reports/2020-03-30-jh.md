Ok, so as for the training protocol, I’ve ran a bunch of training experiments on CO2 with the default ansatz, varying the training protocol while keeping the model fixed. I didn’t really find anything too much useful, but anyway here’s what I found:

- Embeddings—whether frozen or trained, it doesn’t seem to make any difference. Earlier I thought freezing them may allow larger learning rates, but this isn’t the case
- Momentum—isn’t very important. It does make training a bit more stable, but on good runs, training without momentum can be as fast and as good as with momentum. Subsequently, the precise value  of the momentum is inconsequential
- Preconditioning—this is crucial. SGD is horrible compared to Adam. The preconditioning part of Adam does all the work. Adam without momentum is as good as the default Adam. Adam can be thought of as preconditioning the gradient with a square-root of the inverse of a diagonal approximation of the empirical Fisher matrix. The natural gradient (basis for KFAC) would suggest to use just he inverse, without the square-root. I’ve tried that, and it works almost as good as Adam for the energy (but still slightly worse), but results in wave functions with significantly higher variance. No clue why, and this connection between Adam and natural gradient is discussed only superficially in the literature.
- Preconditioner running average—Adam has a large running window by default (`beta2` = 0 .999), which is not efficient in our case (I guess because we use large batches?). 0.9 results in faster training (this is the same value as the default for `beta1` which is essentially momentum)
- Learning rate—the wave function consistently blows up once learning rate goes above 6e-3. This can be found out easily by a ramp-up test in a single run: the learning rate is increased exponentially from, say, 1e-5, until the training blows up. Larger learning rates results in better energies
- Regularization—I’ve tried L2 (which requires using AdamW, because the L2 regularization in standard Adam is broken), and it anecdotally does seem to make the training a bit more reproducible and stable w.r.t. blow-ups, but it does not prevent them
- Learning rate scheduling—I’ve tested different cyclic policies, which are all based on the idea of starting training at a low temperature (learning rate), ramping it up, and then cooling off again, then possibly continue with repeating this cycle. This does enable going to somewhat larger maximal learning rates (I could push as far as 10e-3, which almost always blows up the network immediately when training is started at this learning rate), but unfortunately it doesn’t seem to improve the final accuracy. It’s almost as if the maximal learning rate that does not blow up the network is also where the final accuracy as a function of the learning rate plateaus. This one was almost frustrating—seeing the learning rate running all over the place in different patterns, yet the training curves are almost unchanged

So I have the impression now that there isn’t any low-hanging fruit on the training-protocol side. It’s just how it is. So I’ll just go with the most stable variant (AdamW, beta2=0.9, weight_decay=0.01, starting with a lower learning rate and going up later) , and will scan the model hyperparameters.

As a reminder, the best current number with a single determinant is 93% correlation energy. (And multiple determinants done in the way we’ve been doing it before introducing the multi-backflow didn’t help.)

P.S. As for KFAC, I’ve given up again. Unlike the last time, I now know that my implementation really *should* work, because I’m generating parameter updates that have the correct KL variance, and increase the expected loss change (`dL * dheta`) by orders of magnitude. So at least for deterministic gradient descent, this just should work. But in real life it doesn’t. And then if I bring the `dL * dtheta` down, the training starts to resemble the Adam training with the non-square-root preconditioner (which is worse than Adam for some reason). So I don’t really know.