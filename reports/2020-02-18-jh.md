Ok, so I finally have some new results for CO2. I’ve run a 3^4 hyperparameter scan, on learning rate, batch size, epoch size (the frequency of updating the “sampling” wave function to be the current optimized wave function), and decorrelation length (sampling steps between samples that are actually used for training)

All was done with a single determinant, with backflow, otherwise identical ansatz to the paper
some training runs are still in progress, the best result I got so far is 90(1)% correlation energy, I think I’ll get some percent points lower still. then a few extra determinants should give another improvement, so I think this is promising

As for the hyperparameters:

- decorrelation length must be simply large enough so that the training samples are practically decorrelated. for CO2, this turns out to be 20 steps. having a lower one, such as 10, makes the training noticably less efficient
- the epoch size is most complicated. I’ve tried 3, 5, 8, and 5 is a clear optimum in terms of training efficiency. once all the runs are done, I’ll have a look how this correlates with the importance sampling weights
- I was surprised to find out that the training is independent of the batch size in the range I’ve tried (1k, 2k, 4k). which is great, because clearly smaller batch sizes are more cost effective. I’ll try to go even lower

![Screen Shot 2020-02-18 at 17.37.10](2020-02-18-jh.assets/Screen%20Shot%202020-02-18%20at%2017.37.10.png)

- I’ve tried learning rates of 0.0003, 0.001, 0.003 and convincingly the larger the better. not only in how fast we converge, but more importantly how low. so I would clearly want to go to even larger learning rates, but then I observe that the wave function often blows up in the initial few steps. but that’s also where the gradients on the parameters are largest
  
  ![Screen Shot 2020-02-18 at 17.29.36](2020-02-18-jh.assets/Screen%20Shot%202020-02-18%20at%2017.29.36.png)
  
  So that has me thinking: don’t people do something like a burn-in phase for optimizers with memory such as Adam? intuitively, I would want the optimizer to first learn the individual parameter’s learning rate coefficients when doing safe small steps, and only then ramp up the learning rate. is this reasonable to try?

