ok, so here’s a thorough resampling of a single training run. horizontal lines are again 0%, 90%, 99%, 100% of correlation energy. light grey is plot of every single training step (energy mean over batch, batch size of 1000), grey an exponential moving average, black a smoother moving average. black roughly corresponds to what I usually use in tensorboard to track training (although I just found that tensorboard does a shit-ton of things when smoothing, so I don’t really know). the blue and yellow marks show the energy with 67% uncertainty margins when properly resampled at that point in training (two independent sampling runs, and you can see that the uncertainty margins hold most of the time). the grey dashed lines show the expected 67% uncertainty margin for a single-batch energy mean, and that seems to hold as well

overall, it seems that at later stages, the true sampled energy goes more or less consistently (and slowly) down, but this is hard to see from the moving average when training (although it can actually be seen on the energy *variance* when training. but the fluctuations in the variance do not correspond to the fluctuations in the sampled energy)

in both sampling runs, the model at 4500th step is the best one, at 96.0(3)% of correlation energy. you can actually see the dip in the blue/yellow marks. but this cannot be discerned from the moving average. so to find the best model, one has to do the sampling

I would also point out that unlike in normal ML scenarios, where picking the best model from a training run is sort of cheating, because it can lead to worse generalization, here the sampling is the ultimate test of generalization, so there’s no danger in picking the best model (and as can be seen from the two independent sampling runs, it is reproducibly the best model)

![Unknown](2020-03-31-jh.assets/Unknown.png)